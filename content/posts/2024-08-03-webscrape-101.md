+++
title = 'Data Collection - Webscraping 101'
date = 2024-08-03T11:50:44+08:00
draft = true
summary = "This is a draft for sharing on data collection and webscraping techniques."
tags = [
  "api",
  "beautifulsoup",
  "playwright",
  "selenium",  
  "webscraping"
]
categories = [
  "portfolio"
]
showToc = true
+++



- Talk about data collection
- Talk about number of ways to webscrape.

We want to automate data collection. [Kaggle](https://www.kaggle.com/datasets) is a good place to look for datasets. There is also [reddit](https://www.reddit.com/r/datasets/) but often more of people requesting for datasets but could have some suggestions on where to get them. Or can use [Google Dataset Search](https://datasetsearch.research.google.com/) which is basically Google's dedicated search engine for datasets; although I find not so good.

Also talk about open-sourced data, particularly like in the US. Various states have their own data portal. Singapore has its own [data portal](https://beta.data.gov.sg/) but not a lot of good datasets.

Then we talk about what if the dataset that you want is not readily available, in the sense that the dataset could easily be downloaded in a structured format. 

In this article, I will broadly cover the high level overview of my experiences with data collection and webscraping and not a detailed implementation of the full webscrape workflow. The tools that I introduce here, such as Beautiful Soup, Selenium and Playwright, have good documentations to guide your implementations subsequently. Neverthless, I am planning to work on a full webscrape demo code in the near future. 

***

## API

There could be community-built databases or organizations that provide access to their databases. Most of the time offer via Application Programming Interface (API). API is a server that you can use to retrieve and send data to using code. APIs are most commonly used to retrieve data. When we want to receive data from an API, we need to make a request.

Good practice to always check the status code after API call. Common status codes are:

> - 200: Everything went okay, and the result has been returned (if any).
> - 301: The server is redirecting you to a different endpoint. This can happen when a company switches domain names, or an endpoint name is changed.
> - 400: The server thinks you made a bad request. This can happen when you don’t send along the right data, among other things.
> - 401: The server thinks you’re not authenticated. This happens when you don’t send the right credentials to access an API.
> - 403: The resource you’re trying to access is forbidden: you don’t have the right permissions to see it.
> - 404: The resource you tried to access was not found on the server.
> - 503: The server is not ready to handle the request.

Return result usually in formatted structure, for example json.

One example of a community-built database is [The Movie Database (TMDB)](https://developer.themoviedb.org/docs/getting-started) which provides information on movies, TV shows, and actors/actresses. It includes details such as cast and crew information, plot summaries, reviews, ratings and other rich metadata. Most communities or organizations that provide API services usually have a ["Try It"](https://developer.themoviedb.org/reference/intro/getting-started) page for you to play around with the API.

In this simple example, the aim is to extract the movie title, average rating and character name, in which Tom Cruise had appeared in.

```python
import requests
import json
import pandas as pd

# input your api key
# note: this is definitely not the right practice for production level code
# but for simiplicity we will do it here
api_key = '<your-api-key>'

# do a search using the API to find the id for Tom Cruise
actor = 'tom cruise'
url = "https://api.themoviedb.org/3/search/person?query={}&include_adult=false&language=en-US&page=1&api_key={}".format(actor, api_key)

response = requests.get(url)
print(response.status_code)

print(response.json()) # in the response we can see that the id for Tom Cruise is 500

person_id = '500'
url = "https://api.themoviedb.org/3/person/{}/movie_credits?language=en-US&api_key={}".format(person_id, api_key)

response = requests.get(url)
print(response.status_code)

tom_cruise = response.json()

# parse the json data into a pandas dataframe
# in this case, we retrieve just the movie title, vote and character name information
movie_list = []

for m in tom_cruise['cast']:
    movie = m['title']
    vote = m['vote_average']
    char = m['character']
    movie_list.append([movie, vote, char])

df = pd.DataFrame(movie_list, columns = ['title', 'vote_average', 'character'])
```

{{< figure src="/images/tom-cruise.png" width="400" alt="Tom Cruise movies" caption="A DataFrame Example of Tom Cruise movies" class="markdown-image">}}

***

## Static Website - Requests and Beautiful Soup

In general, there are two types of websites: static and dynamic. Static websites have fixed content that does not change based on user interactions and typically each page has a unique URL. Dynamic websites load content based on user interactions and the base URL may remain the same while content is loaded dynamically. Let's start with the simpler case of webscraping on static websites.

Scrape source, parse the output using Beautiful Soup. [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML. This allows us to easily search and iterate over with Python loops to retrieve our desired data.

```python
from bs4 import BeautifulSoup
import requests

url = 'https://boardgamegeek.com/browse/boardgame/page/1'
response = requests.get(url)
print(response.status_code)
```

Now if you just examine the content from the reponse, you would notice that the content is simply a block of string.

```python
# note you cannot use response.json() as the return type is not json format
print(response.content)
```

{{< figure src="/images/unparsed-content.png" width="800" alt="Unparsed content" caption="Unparsed content" class="markdown-image">}}

In order to format the content proper so that you can traverse the content structure, there is a need to parse the content using a parser.

```python
# can also use html.parser instead of lxml
soup = BeautifulSoup(response.content, "lxml")
print(soup)
```

{{< figure src="/images/parsed-content.png" width="800" alt="Parsed content" caption="Parsed content" class="markdown-image">}}

With the entire HTML source retrieved, now you have to identify the specific element that you want. Now you need to right-click and inspect.


{{< figure src="/images/right-click-inspect.png" width="800" alt="Inspect element" class="markdown-image">}}
{{< figure src="/images/inspect-result.png" width="1000" alt="Inspect element" class="markdown-image">}}
{{< figure src="/images/inspect-dom.png" width="500" alt="Inspect element" class="markdown-image">}}


This approach identifies and uses the types of tags to retrieve the elements. You can also identify tags with certain attributes such as `id`, `class` or `css`. In the below example, we find all elements that have the `<a>` tag with class `primary`. Note that the keyword argument has to be `class_` as `class` is a reserved keyword in Python.

```python
soup.find_all('a', class_='primary')
```

{{< figure src="/images/a-primary-find-all.png" width="1200" alt="Find all" caption="Using .find_all to retrieve elements" class="markdown-image">}}

```python
games = []

# extract boardgame URL
for i in range(5):
    url = 'https://boardgamegeek.com/browse/boardgame/page/' + str(i+1)
    soup = BeautifulSoup(response.content, "lxml")
    boardgame_id = soup.find_all('a', class_='primary')
    
    # retrieve the desired elements; can also use b.get('href') instead of b['href']
    for b in boardgame_id:
        games.append([b.text,b['href']])

df = pd.DataFrame(games, columns = ['boardgame_name', 'boardgame_link'])
```

{{< figure src="/images/board-game-df.png" width="450" alt="Boardgame dataframe" caption="Format into pandas dataframe" class="markdown-image">}}

***

## Dynamic Website - Selenium/Playwright and Beautiful Soup

For dynamic websites, it is more tricky as the content that you may want to scrape may only be loaded and visible in the HTML through user interactions. For example, selecting a value from a dropdown, clicking on a button, or hovering over a content. We will need to make use of tools to simulate user interactions. 

To do so, we can make use of [Selenium](https://selenium-python.readthedocs.io/) or [Playwright](https://playwright.dev/python/docs/intro) which are Python packages (note that they also support other programming languages) used for automating web browsers to perform tasks. 

One of such is webscraping to extract useful data and information that may be otherwise unavailable.

The general workflow is as such:
> 1. Launch the browser
> 2. Perform user actions. This could include logging-in, selecting value from dropdown, clicking on button
> 3. Once the desired information are shown on the webpage, save the HTML page source
> 4. Close the browser
> 5. Parse the HTML page source using Beautiful Soup, extract and transform the desired information

In the below 

In Selenium, to interact with the page, the following are common methods:

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

# initialize
browser = webdriver.Chrome()
url = "https://website-url-here"
browser.get(url)

# locate the element
element = browser.find_element(By.ID, "passwd-id")
element = browser.find_element(By.NAME, "passwd")
element = browser.find_element(By.CLASS_NAME, "class-name")
element = browser.find_element(By.XPATH, "//input[@id='passwd-id']")
element = browser.find_element(By.CSS_SELECTOR, "input#passwd-id")

# send value to the element, e.g. if it is a input field
element.send_keys("my-password")

# if it is a submit button, you can click on it
element.click()
```
You can do the same for Playwright. However, note that Playwright in Python supports two variations - synchronous and asynchronous. If you are using Jupyter notebook, note that it uses asyncio event loops and hence you will not be able to use the synchronous variation (below example implementation is synchronous).

```python
# initialize
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch()
    context = browser.new_context()
    page = context.new_page()
    page.goto("https://website-url-here")

    # locate the element
    element = page.locator("id=passwd-id")
    element = page.locator("name=passwd")
    lement = page.locator(".class-name")  # use css selector for class name; as Playwright does not explicitly support class name
    element = page.locator("xpath=//input[@id='passwd-id']")
    element = page.locator("css=input#passwd-id")

    # send input value to the element, e.g. if it is a fill box
    element.fill("my-password")

    # if it is a submit button, you can click on it
    element.click()
```

To look for the attribute, e.g. `id` or `name`, use the inspect method as shown in the earlier section. More often than not, the preference is to use the `By.ID` method as the `id` is a unique identifier for an HTML element. 

Without an appropriate identifier, we could use the `XPATH` assessed through the Chrome developer tool. The xpath will look something like `//*[@id="app"]/div/div/div[2]/div/ul/li[2]/a`.

{{< figure src="/images/inspect-copy-xpath1.png" width="450" alt="Element xpath" class="markdown-image">}}
{{< figure src="/images/inspect-copy-xpath2.png" width="450" alt="Element xpath" caption="Copy the xpath of the element" class="markdown-image">}}

***

## Other Notes on Selenium/Playwright

### Wait, not Sleep

When a page is being loaded by the browser, elements within the page may load at different intervals. This may result in an `ElementNotVisibleException` exception. In the past, I simply peppered my code with multiple `time.sleep()` in a blind attempt to wait for the page to load completely. Using `WebDriverWait` is more efficient as it has regular interval polling and will proceed once the condition is met rather than the full sleep time.

A simple example of how the `WebDriverWait` can be implemented:

```python
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# define wait for 10 seconds before raising exception if element not found
wait = WebDriverWait(browser, 10)  # timing is in seconds
wait.until(EC.element_to_be_clickable((By.ID, 'someid')))
```

For Playwright,

```python
page.wait_for_selector("id=someid", state="visible", timeout=10000)  # timing is in milliseconds
```

### Where is my Download?

In the case of Selenium, a user interaction that trigger a file download will result in the file automatically be saved in the default download folder.

For Playwright, downloads are saved to a temporary folder, and there is a need to obtain the download URL, file name and payload stream using the `Download` object.

```python
# Start waiting for the download
with page.expect_download() as download_info:

    # Perform the action that initiates download
    page.get_by_text("Download file").click()

download = download_info.value

# Wait for the download process to complete and save the downloaded file somewhere
download.save_as("/path/to/save/at/" + download.suggested_filename)
```

### Switching Tabs

At times, clicking on an element results in a new pop-up tab on the browser. There is a need to explictly switch the browser/context to the new tab.

Below is an example code snippet on how it can be done in Selenium.

```python
browser.switch_to.window(browser.window_handles[1])
```

Below is an example code snippet on how it can be done in Playwright.

```python
# method #1: use context.pages
context.wait_for_event("page")
new_page = context.pages[1]
new_page.wait_for_selector(".job-title", state="visible")  # use visible element to confirm element loaded

# method #2: use context.expect_page
# in initialization, context = browser.new_context() and page = context.new_page()
with context.expect_page(timeout=60000) as new_page_info:  # timeout is in ms
  page.wait_for_selector("id=newpageid", state="visible", timeout=60000)
  page.locator("id=newpageid").click()

new_page = new_page_info.value
new_page.wait_for_selector(".job-title", state="visible")  # use visible element to confirm element loaded
```